{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from matplotlib.image import imread\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from __future__ import print_function\n",
    "from ladder_net import get_ladder_network_fc\n",
    "\n",
    "# each image dimension is (128, 384). After segmentation the following will be the dimensions of each character.\n",
    "charHeight = 128\n",
    "charWidth = 128\n",
    "numImages = 50000\n",
    "\n",
    "vectorLength = 16384\n",
    "imgPath = \"SoML-50/SoML-50/data/\"\n",
    "csvPath = \"SoML-50/SoML-50/annotations.csv\"\n",
    "manual_path = \"manual.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "class Manual:\n",
    "    def __init__(self, path):\n",
    "        self.df = pd.read_csv (path)\n",
    "        self.df.set_index ('Image', inplace =True)\n",
    "  \n",
    "    def getOperator (self,index):\n",
    "        index = int (index) \n",
    "        if (1 <= index <= 1000):\n",
    "            char = str(self.df['operator'][index])\n",
    "            if (char == '+'):\n",
    "                return 0\n",
    "            elif (char == '-'):\n",
    "                return 1 \n",
    "            elif (char == '*'):\n",
    "                return 2\n",
    "            elif (char == '/'):\n",
    "                return 3\n",
    "            else: \n",
    "                raise Exception (\"Invalid operator at index : \" + str(index))\n",
    "        raise Exception(\"Index is limited to [1,1000] only. Given: \" + str(index))\n",
    "    \n",
    "    def getOp1 (self,index):\n",
    "        index = int (index) \n",
    "        if (1 <= index <= 1000):\n",
    "            return str(self.df['op1'][index])\n",
    "        raise Exception(\"Index is limited to [1,1000] only. Given: \" + str(index))\n",
    "    \n",
    "    def getOp2 (self,index):\n",
    "        index = int (index) \n",
    "        if (1 <= index <= 1000):\n",
    "            return str(self.df['op2'][index])\n",
    "        raise Exception(\"Index is limited to [1,1000] only. Given: \" + str(index))\n",
    "\n",
    "manual = Manual (manual_path)\n",
    "print (manual.getOperator (100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "class Annotations:\n",
    "    def __init__ (self, path):                                                           \n",
    "        self.df = pd.read_csv (path)\n",
    "        self.df.set_index ('Image', inplace =True)\n",
    "        \n",
    "    def normalizeImg (self,npArr):\n",
    "        return npArr / 255.0\n",
    "    \n",
    "    def getPathOfImg (self,index):\n",
    "        return (imgPath + str (index) + \".jpg\")\n",
    "    \n",
    "    def getLabelOfImg (self,index):\n",
    "        return (self.df.loc[(str(index) + '.jpg')]['Label'])\n",
    "\n",
    "    def getValueOfImg (self, index):\n",
    "        return (int(self.df.loc[(str(index) + '.jpg')]['Value']))\n",
    "    \n",
    "    def getSegmentedVectors (self,index):\n",
    "        \"\"\" This function returns a numpy array of the three character images of shape (128,128) present in index.jpg. \n",
    "            Also, the operator is always present at [0] and other two operands at [1] and [2] in the order in which the operator has to be applied.\"\"\"\n",
    "\n",
    "        image = imread (self.getPathOfImg(index))\n",
    "        label = self.getLabelOfImg (index)\n",
    "        if (label == 'prefix'):\n",
    "            charArray = np.array ([image[:, 0:charWidth],image[:, charWidth:(2*charWidth)],image[:, (2*charWidth):]])\n",
    "        elif (label == 'postfix'):\n",
    "            charArray = np.array ([image[:, (2*charWidth):],image[:, 0:charWidth],image[:, charWidth:(2*charWidth)]])\n",
    "        else:\n",
    "            charArray = np.array ([image[:, charWidth:(2*charWidth)],image[:, 0:charWidth],image[:, (2*charWidth):]])\n",
    "        charArray = np.array ([self.normalizeImg(i) for i in charArray])\n",
    "        return charArray\n",
    "\n",
    "\n",
    "annotations = Annotations (csvPath)\n",
    "\n",
    "print (annotations.getSegmentedVectors (5))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN: \n",
    "    def __init__ (self,manualPath, csvPath):\n",
    "        self.manual = Manual (manualPath)\n",
    "        self.annotations = Annotations (csvPath)\n",
    "        \n",
    "        self.inputSize = (128*128)\n",
    "        self.numOperators = 4\n",
    "        self.numOperands = 10\n",
    "        self.numBinaryClasses = 2\n",
    "        self.epochs = 10\n",
    "        \n",
    "        # divide dataset into 10 sets. labeled dataset into 10 of sizes 100 each. and unlabeled into 10 of 4000 size each. rest use for testing (9000)\n",
    "        self.numMiniBatches = 10\n",
    "        \n",
    "        #initialze all three models\n",
    "        operatorModel = get_ladder_network_fc (layer_sizes=[self.inputSize, 1000, 500, 250, 250, 250, self.numOperators])\n",
    "        operandModel = get_ladder_network_fc (layer_sizes=[self.inputSize, 1000, 500, 250, 250, 250, self.numOperands])\n",
    "        binaryModel = get_ladder_network_fc (layer_sizes=[self.inputSize, 1000, 500, 250, 250, 250, self.numBinaryClasses])\n",
    "        \n",
    "    def getLabeledMiniBatch (self,batchNum): \n",
    "        '''returns a tuple x_labeled and y_labeled'''\n",
    "        if (batchNum < 0 or batchNum >= self.numMiniBatches):\n",
    "            raise ValueError (\"batchNum should be between -1 and\" + str(self.numMiniBatches))\n",
    "        \n",
    "        numElements = 1000 // self.numMiniBatches\n",
    "        \n",
    "        x_operator_labeled = np.empty (shape = [numElements, 128,128])\n",
    "        y_operator_labeled = np.empty (shape = [numElements,1])\n",
    "        x_operand_labeled = np.empty (shape = [2*numElements,128,128])\n",
    "        y_operand_labeled = np.empty (shape = [2*numElements,1])\n",
    "        x_binary_labeled = np.empty (shape = [3 * numElements, 128,128])\n",
    "        y_binary_labeled = np.empty (shape = [3 * numElements, 1])\n",
    "        \n",
    "        for i in range (batchNum * numElements + 1,(batchNum + 1) * numElements + 1):  \n",
    "            segments = self.annotations.getSegmentedVectors (i)\n",
    "            x_operator_labeled[(i-1)%numElements] = segments[0]\n",
    "            y_operator_labeled[(i-1)%numElements] = self.manual.getOperator(i)\n",
    "            \n",
    "            x_operand_labeled[(i-1)%numElements] = segments[1]\n",
    "            x_operand_labeled[((i-1)%numElements) + numElements] = segments[2]\n",
    "            \n",
    "            y_operand_labeled[(i-1)%numElements] = self.manual.getOp1 (i)            \n",
    "            y_operand_labeled[((i-1)%numElements) + numElements] = self.manual.getOp2 (i)\n",
    "            \n",
    "            x_binary_labeled[(i-1)%numElements] = segments[0]            \n",
    "            x_binary_labeled[((i-1)%numElements) + numElements] = segments[1]            \n",
    "            x_binary_labeled[((i-1)%numElements) + (2*numElements)] = segments[2]\n",
    "            \n",
    "            y_binary_labeled[(i-1)%numElements] =  0           \n",
    "            y_binary_labeled[((i-1)%numElements) + numElements] =  1          \n",
    "            y_binary_labeled[((i-1)%numElements) + (2*numElements)] =  1 \n",
    "                      \n",
    "        y_operator_labeled = to_categorical (y_operator_labeled, num_classes = 4)\n",
    "        y_operand_labeled = to_categorical (y_operand_labeled, num_classes = 10)\n",
    "        y_binary_labeled = to_categorical (y_binary_labeled, num_classes = 2)\n",
    "        \n",
    "        x_binary_labeled , y_binary_labeled = shuffle (x_binary_labeled, y_binary_labeled) \n",
    "        \n",
    "        return (x_operator_labeled, y_operator_labeled, x_operand_labeled, y_operand_labeled, x_binary_labeled, y_binary_labeled)\n",
    "        \n",
    "    \n",
    "    def getUnlabeledMiniBatch (self, batchNum):\n",
    "        '''returns a tuple x_unlabeled and y_unlabeled'''\n",
    "        if (batchNum < 0 or batchNum >= self.numMiniBatches):\n",
    "            raise ValueError (\"batchNum should be between -1 and\" + str(self.numMiniBatches))\n",
    "        \n",
    "        numElements = 40000 // self.numMiniBatches\n",
    "        \n",
    "        x_operator_unlabeled = np.empty (shape = [numElements, 128,128])\n",
    "        x_operand_unlabeled = np.empty (shape = [2*numElements,128,128])\n",
    "        x_binary_unlabeled = np.empty (shape = [3 * numElements, 128,128])\n",
    "        \n",
    "        for i in range (batchNum * numElements + 1001,(batchNum + 1) * numElements + 1001):  \n",
    "            segments = self.annotations.getSegmentedVectors (i)\n",
    "            \n",
    "            x_operator_unlabeled[(i-1001)%numElements] = segments[0]\n",
    "            \n",
    "            x_operand_unlabeled[(i-1001)%numElements] = segments[1]\n",
    "            x_operand_unlabeled[((i-1001)%numElements) + numElements] = segments[2]\n",
    "                   \n",
    "            x_binary_unlabeled[(i-1001)%numElements] = segments[0]            \n",
    "            x_binary_unlabeled[((i-1001)%numElements) + numElements] = segments[1]            \n",
    "            x_binary_unlabeled[((i-1001)%numElements) + (2*numElements)] = segments[2]\n",
    "        \n",
    "        \n",
    "        x_binary_unlabeled = shuffle(x_binary_unlabeled)\n",
    "        \n",
    "        return (x_operator_unlabeled, x_operand_unlabeled, x_binary_unlabeled)\n",
    "        \n",
    "    \n",
    "    def trainOneEpoche (self, epochNum):\n",
    "        for batchNum in range (self.numMiniBatches):\n",
    "            print (\"Epoche = \" + str(epochNum), \"batchNum = \" + str(batchNum))\n",
    "            \n",
    "             \n",
    "            \n",
    "        pass\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "(1000, 128, 128) (2000, 128, 128)\n",
      "(1000, 128, 128, 1) (2000, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "operatorX = np.empty (shape = [1000, 128, 128])\n",
    "operandX = np.empty (shape = [2000, 128, 128])\n",
    "\n",
    "\n",
    "tempOperatorY = [manual.getOperator(i) for i in range (1, 1001)]\n",
    "tempOperand1Y = [manual.getOp1(i) for i in range (1, 1001)]\n",
    "tempOperand2Y = [manual.getOp2(i) for i in range (1, 1001)]\n",
    "tempOperand1Y.extend (tempOperand2Y)\n",
    "\n",
    "operatorY = np.array (to_categorical(tempOperatorY))\n",
    "operandY = np.array (to_categorical(tempOperand1Y))\n",
    "\n",
    "print (operandY[1002])\n",
    "\n",
    "for img in range (1, 1001):\n",
    "    segments = annotations.getSegmentedVectors (img)\n",
    "    operatorX[img-1] = segments[0]\n",
    "    operandX[img-1] = segments[1]\n",
    "    operandX[img+999] = segments[2]\n",
    "     \n",
    "print (operatorX.shape, operandX.shape)\n",
    "operatorX = np.expand_dims (operatorX, -1)\n",
    "operandX = np.expand_dims (operandX, -1)\n",
    "print (operatorX.shape, operandX.shape)\n",
    "# make sure images have shape (128,128,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator_clf = keras.Sequential (\n",
    "    [\n",
    "        keras.Input (shape = (128,128,1)),\n",
    "        layers.Conv2D(32, kernel_size = (3,3), activation = \"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(4, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# operator_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 11s 604ms/step - loss: 1.8395 - accuracy: 0.4132 - val_loss: 0.7236 - val_accuracy: 0.8700\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 15s 764ms/step - loss: 0.7093 - accuracy: 0.7737 - val_loss: 0.3983 - val_accuracy: 0.8900\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 15s 847ms/step - loss: 0.4113 - accuracy: 0.8585 - val_loss: 0.3739 - val_accuracy: 0.8800\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 12s 663ms/step - loss: 0.2809 - accuracy: 0.9201 - val_loss: 0.2984 - val_accuracy: 0.9000\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 14s 698ms/step - loss: 0.1796 - accuracy: 0.9488 - val_loss: 0.2624 - val_accuracy: 0.9200\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 15s 842ms/step - loss: 0.1466 - accuracy: 0.9660 - val_loss: 0.2844 - val_accuracy: 0.8900\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 11s 617ms/step - loss: 0.0960 - accuracy: 0.9738 - val_loss: 0.2888 - val_accuracy: 0.9100\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 15s 753ms/step - loss: 0.0757 - accuracy: 0.9846 - val_loss: 0.3086 - val_accuracy: 0.9200\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 15s 877ms/step - loss: 0.0615 - accuracy: 0.9878 - val_loss: 0.2685 - val_accuracy: 0.9300\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 15s 848ms/step - loss: 0.0511 - accuracy: 0.9904 - val_loss: 0.3159 - val_accuracy: 0.9300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4be936f370>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "epochs = 10\n",
    "\n",
    "operator_clf.compile (loss = \"categorical_crossentropy\",optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "operator_clf.fit(operatorX,operatorY, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07282477617263794\n",
      "Test accuracy: 0.9829999804496765\n"
     ]
    }
   ],
   "source": [
    "score = operator_clf.evaluate (operatorX[0:1000], operatorY[0:1000], verbose = 0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "operand_clf = keras.Sequential (\n",
    "    [\n",
    "        keras.Input (shape = (128,128,1)),\n",
    "        layers.Conv2D(64, kernel_size = (3,3), activation = \"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# operand_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36/36 [==============================] - 19s 525ms/step - loss: 2.3278 - accuracy: 0.2022 - val_loss: 1.4673 - val_accuracy: 0.5750\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 23s 644ms/step - loss: 1.2308 - accuracy: 0.6181 - val_loss: 1.1278 - val_accuracy: 0.6500\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 23s 623ms/step - loss: 0.7055 - accuracy: 0.7947 - val_loss: 0.8745 - val_accuracy: 0.7200\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 21s 596ms/step - loss: 0.3399 - accuracy: 0.9200 - val_loss: 0.8079 - val_accuracy: 0.7750\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 21s 596ms/step - loss: 0.1688 - accuracy: 0.9653 - val_loss: 0.9604 - val_accuracy: 0.7650\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 22s 601ms/step - loss: 0.1124 - accuracy: 0.9726 - val_loss: 0.9374 - val_accuracy: 0.8100\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 21s 588ms/step - loss: 0.0675 - accuracy: 0.9864 - val_loss: 0.9629 - val_accuracy: 0.7950\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 21s 590ms/step - loss: 0.0357 - accuracy: 0.9945 - val_loss: 1.0327 - val_accuracy: 0.7900\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 22s 599ms/step - loss: 0.0277 - accuracy: 0.9964 - val_loss: 1.0552 - val_accuracy: 0.7750\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 24s 658ms/step - loss: 0.0239 - accuracy: 0.9972 - val_loss: 1.2353 - val_accuracy: 0.7750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4b942be310>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operand_clf.compile (loss = \"categorical_crossentropy\",optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "operand_clf.fit(operandX,operandY, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2557852864265442\n",
      "Test accuracy: 0.9539999961853027\n"
     ]
    }
   ],
   "source": [
    "OperandScore = operand_clf.evaluate (operandX[1000:2000], operandY[1000:2000], verbose = 0)\n",
    "print(\"Test loss:\", OperandScore[0])\n",
    "print(\"Test accuracy:\", OperandScore[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "binaryX = np.concatenate ((operatorX, operandX))\n",
    "tempBinary0Y = [0 for i in range (1000)]\n",
    "tempBinary1Y = [1 for i in range (2000)]\n",
    "tempBinary0Y.extend (tempBinary1Y)\n",
    "binaryY = np.array (to_categorical (tempBinary0Y))\n",
    "\n",
    "binaryX, binaryY = shuffle (binaryX, binaryY)\n",
    "print (binaryY[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_clf = keras.Sequential (\n",
    "    [\n",
    "        keras.Input (shape = (128,128,1)),\n",
    "        layers.Conv2D(64, kernel_size = (3,3), activation = \"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(2, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# binary_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54/54 [==============================] - 27s 495ms/step - loss: 0.5987 - accuracy: 0.6974 - val_loss: 0.2780 - val_accuracy: 0.8900\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 28s 521ms/step - loss: 0.2506 - accuracy: 0.9068 - val_loss: 0.1780 - val_accuracy: 0.9333\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 34s 621ms/step - loss: 0.1546 - accuracy: 0.9429 - val_loss: 0.1597 - val_accuracy: 0.9400\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 34s 631ms/step - loss: 0.1108 - accuracy: 0.9526 - val_loss: 0.1177 - val_accuracy: 0.9633\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 32s 599ms/step - loss: 0.0843 - accuracy: 0.9718 - val_loss: 0.1084 - val_accuracy: 0.9733\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 33s 603ms/step - loss: 0.0525 - accuracy: 0.9862 - val_loss: 0.0995 - val_accuracy: 0.9733\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 35s 642ms/step - loss: 0.0527 - accuracy: 0.9817 - val_loss: 0.1246 - val_accuracy: 0.9600\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 34s 624ms/step - loss: 0.0362 - accuracy: 0.9883 - val_loss: 0.0945 - val_accuracy: 0.9733\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 32s 601ms/step - loss: 0.0383 - accuracy: 0.9860 - val_loss: 0.1111 - val_accuracy: 0.9767\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 33s 609ms/step - loss: 0.0242 - accuracy: 0.9960 - val_loss: 0.1164 - val_accuracy: 0.9767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4b582b7490>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_clf.compile (loss = \"binary_crossentropy\",optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "binary_clf.fit(binaryX,binaryY, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.02167610079050064\n",
      "Test accuracy: 0.9959999918937683\n"
     ]
    }
   ],
   "source": [
    "binaryScore = binary_clf.evaluate (binaryX[0:3000], binaryY[0:3000], verbose = 0)\n",
    "print(\"Test loss:\", binaryScore[0])\n",
    "print(\"Test accuracy:\", binaryScore[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgIndex = [ i for i in range (1001, numImages+1)]\n",
    "train_set, test_set = train_test_split (imgIndex, test_size= 0.05, random_state= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls gdrive/MyDrive\n",
    "!unzip gdrive/MyDrive/SoML-50.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73bd737fbce9fefea06c333d19b0e4dd23dd32e9701a28ba35ccf8a2425e105d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('my_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}